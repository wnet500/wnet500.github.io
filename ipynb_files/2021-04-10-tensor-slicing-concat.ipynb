{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd04dacac18bb4c8d034963535e678c162019d7d9af8bc1af267bd97f92f0e2918c",
   "display_name": "Python 3.8.5 64-bit ('torch_intel': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Pytorch Tensor slicing & concatenation\n",
    "- 인공지능을 학습 시킬 때, 목적에 따라 데이터셋과 네트워크 구조를 원하는대로 다루는 것은 중요하다.\n",
    "- 이 포스팅은 Tensor를 자르고 붙이는 방법에 대해 정리해 보았다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([3, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.FloatTensor([[[1, 2],\n",
    "                        [3, 4]],\n",
    "                       [[5, 6],\n",
    "                        [7, 8]],\n",
    "                       [[9, 10],\n",
    "                        [11, 12]]])\n",
    "print(x.size())"
   ]
  },
  {
   "source": [
    "## 1. Indexing\n",
    "### numpy 문법\n",
    "- numpy와 같은 방법으로 인덱싱이 가능하다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[1., 2.],\n        [3., 4.]])\ntensor([[1., 2.],\n        [3., 4.]])\ntensor([[1., 2.],\n        [3., 4.]])\n"
     ]
    }
   ],
   "source": [
    "print(x[0])\n",
    "print(x[0, :])\n",
    "print(x[0, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([3, 1, 2])\ntorch.Size([3, 1, 2])\n"
     ]
    }
   ],
   "source": [
    "print(x[:, :1, :].size())\n",
    "print(x[:, :-1, :].size())"
   ]
  },
  {
   "source": [
    "- 단순히 데이터 1개를 얻는 것 뿐만 아니라, range를 통해 연속된 데이터를 얻을 수 있다.\n",
    "\n",
    "- `a:b` 는 **a 이상 b 미만**을 의미한다.\n",
    "- (ex.) 1:3 -> 1, 2   \n",
    "  (ex.)  :-1 -> 0, 1, 2, ... ,-2 (마지막을 뺀 나머지) "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[[ 5.,  6.],\n         [ 7.,  8.]],\n\n        [[ 9., 10.],\n         [11., 12.]]]) torch.Size([2, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "print(x[1:3, :, :], x[1:3, :, :].size())"
   ]
  },
  {
   "source": [
    "#### index_select\n",
    "-  중복 추출이 가능하다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([3, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "x = torch.FloatTensor([[[1, 1],\n",
    "                        [2, 2]],\n",
    "                       [[3, 3],\n",
    "                        [4, 4]],\n",
    "                       [[5, 5],\n",
    "                        [6, 6]]])\n",
    "indices = torch.LongTensor([1, 0])\n",
    "\n",
    "print(x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[[3., 3.],\n         [4., 4.]],\n\n        [[1., 1.],\n         [2., 2.]]]) torch.Size([2, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "y = x.index_select(dim=0, index=indices)\n",
    "\n",
    "print(y, y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[[2., 2.],\n         [1., 1.]],\n\n        [[4., 4.],\n         [3., 3.]],\n\n        [[6., 6.],\n         [5., 5.]]]) torch.Size([3, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "y = x.index_select(dim=1, index=indices)\n",
    "\n",
    "print(y, y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[[1., 1.],\n         [2., 2.]],\n\n        [[3., 3.],\n         [4., 4.]],\n\n        [[5., 5.],\n         [6., 6.]]]) torch.Size([3, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "y = x.index_select(dim=-1, index=indices)\n",
    "\n",
    "print(y, y.size())"
   ]
  },
  {
   "source": [
    "## 2. Spliting\n",
    "- `split`과 `chunk`함수를 통해 텐서를 원하는 대로 분리할 수 있다. 다소 헷갈리는 개념이니 확실히 짚고 넘어가면 좋다.\n",
    "\n",
    "- `split`은 **n개씩 묶어서** 분리한다.   \n",
    "   마지막, n개보다 작은 묶음은 그대로 둔다.\n",
    "\n",
    "- `chunck`는 **m 묶음으로** 분리한다.   \n",
    "   마지막, 남은 묶음은 작은 묶음은 그대로 둔다.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[[ 0.,  1.],\n         [ 2.,  3.],\n         [ 4.,  5.]],\n\n        [[ 6.,  7.],\n         [ 8.,  9.],\n         [10., 11.]],\n\n        [[12., 13.],\n         [14., 15.],\n         [16., 17.]],\n\n        [[18., 19.],\n         [20., 21.],\n         [22., 23.]],\n\n        [[24., 25.],\n         [26., 27.],\n         [28., 29.]]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.FloatTensor(range(30)).reshape(5,3,2)\n",
    "print(x)"
   ]
  },
  {
   "source": [
    "### split\n",
    "- 2개씩 묶고, 마지막 1개는 그대로 둔다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[[ 0.,  1.],\n         [ 2.,  3.],\n         [ 4.,  5.]],\n\n        [[ 6.,  7.],\n         [ 8.,  9.],\n         [10., 11.]]]) torch.Size([2, 3, 2]) \n----------------------------------\ntensor([[[12., 13.],\n         [14., 15.],\n         [16., 17.]],\n\n        [[18., 19.],\n         [20., 21.],\n         [22., 23.]]]) torch.Size([2, 3, 2]) \n----------------------------------\ntensor([[[24., 25.],\n         [26., 27.],\n         [28., 29.]]]) torch.Size([1, 3, 2]) \n----------------------------------\n"
     ]
    }
   ],
   "source": [
    "#----- split\n",
    "splits = x.split(2, dim=0) # default dim = 0\n",
    "\n",
    "for s in splits:\n",
    "    print(s, s.size(), \"\\n----------------------------------\")"
   ]
  },
  {
   "source": [
    "### chunk\n",
    "- 총 2묶음으로 묶기 위해 3개, 2개로 묶는다. (묶음: 큰 수 -> 작은 수)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[[ 0.,  1.],\n         [ 2.,  3.],\n         [ 4.,  5.]],\n\n        [[ 6.,  7.],\n         [ 8.,  9.],\n         [10., 11.]],\n\n        [[12., 13.],\n         [14., 15.],\n         [16., 17.]]]) torch.Size([3, 3, 2]) \n----------------------------------\ntensor([[[18., 19.],\n         [20., 21.],\n         [22., 23.]],\n\n        [[24., 25.],\n         [26., 27.],\n         [28., 29.]]]) torch.Size([2, 3, 2]) \n----------------------------------\n"
     ]
    }
   ],
   "source": [
    "#----- chunk\n",
    "chunks = x.chunk(2, dim=0) # default dim = 0\n",
    "\n",
    "for c in chunks:\n",
    "    print(c, c.size(), \"\\n----------------------------------\")"
   ]
  },
  {
   "source": [
    "- `dim=1`일 때, 두 번째 dim을 쪼갠다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "x size:\ntorch.Size([5, 3, 2]) \n-----------------------------------\nchucked sizes:\ntorch.Size([5, 2, 2])\ntorch.Size([5, 1, 2])\n"
     ]
    }
   ],
   "source": [
    "chunks = x.chunk(2, dim=1) # default dim = 0\n",
    "\n",
    "print(\"x size:\")\n",
    "print(x.size(), \"\\n-----------------------------------\")\n",
    "\n",
    "print(\"chucked sizes:\")\n",
    "for c in chunks:\n",
    "    print(c.size())"
   ]
  },
  {
   "source": [
    "## 3. Concatenate & Stack\n",
    "- `cat`과 `stack` 함수를 사용하여 여러 개의 텐서를 이어 붙이고 쌓을 수 있다.\n",
    "- `cat`을 이용하여 여러 개의 텐서를 붙일 수 있다. (dim 변화 없음)\n",
    "- `stack`을 이용하여 여러 개의 텐서를 쌓아 올릴 수 있다. (dim 변화 있음)\n",
    "- **이미지**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([3, 3]) torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "x = torch.FloatTensor([[1, 2, 3],\n",
    "                       [4, 5, 6],\n",
    "                       [7, 8, 9]])\n",
    "y = torch.FloatTensor([[10, 11, 12],\n",
    "                       [13, 14, 15],\n",
    "                       [16, 17, 18]])\n",
    "\n",
    "print(x.size(), y.size())"
   ]
  },
  {
   "source": [
    "### cat (차원 유지)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[ 1.,  2.,  3.],\n        [ 4.,  5.,  6.],\n        [ 7.,  8.,  9.],\n        [10., 11., 12.],\n        [13., 14., 15.],\n        [16., 17., 18.]]) torch.Size([6, 3])\n"
     ]
    }
   ],
   "source": [
    "z = torch.cat([x, y], dim=0) # default dim = 0\n",
    "\n",
    "print(z, z.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[ 1.,  2.,  3., 10., 11., 12.],\n        [ 4.,  5.,  6., 13., 14., 15.],\n        [ 7.,  8.,  9., 16., 17., 18.]]) torch.Size([3, 6])\n"
     ]
    }
   ],
   "source": [
    "z = torch.cat([x, y], dim=1)\n",
    "\n",
    "print(z, z.size())"
   ]
  },
  {
   "source": [
    "### stack (차원 증가)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[[ 1.,  2.,  3.],\n         [ 4.,  5.,  6.],\n         [ 7.,  8.,  9.]],\n\n        [[10., 11., 12.],\n         [13., 14., 15.],\n         [16., 17., 18.]]]) torch.Size([2, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "z = torch.stack([x, y], dim=0) # default dim = 0\n",
    "\n",
    "print(z, z.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[[ 1.,  2.,  3.],\n         [10., 11., 12.]],\n\n        [[ 4.,  5.,  6.],\n         [13., 14., 15.]],\n\n        [[ 7.,  8.,  9.],\n         [16., 17., 18.]]]) torch.Size([3, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "z = torch.stack([x, y], dim=1)\n",
    "\n",
    "print(z, z.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[[ 1., 10.],\n         [ 2., 11.],\n         [ 3., 12.]],\n\n        [[ 4., 13.],\n         [ 5., 14.],\n         [ 6., 15.]],\n\n        [[ 7., 16.],\n         [ 8., 17.],\n         [ 9., 18.]]]) torch.Size([3, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "z = torch.stack([x, y], dim=2)\n",
    "\n",
    "print(z, z.size())"
   ]
  },
  {
   "source": [
    "### 유용한 예제\n",
    "- 리스트에 차곡차곡 쌓은 후, stack을 통해서 차원을 늘려 데이터셋을 구성할 수 있다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[tensor([1., 2.]), tensor([3., 4.])]\n\n-----------------------\n\ntensor([[1., 2.],\n        [3., 4.]]) torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "w = [] + [torch.FloatTensor([1, 2])] + [torch.FloatTensor([3, 4])]\n",
    "print(w)\n",
    "\n",
    "print(\"\\n-----------------------\\n\")\n",
    "\n",
    "w = torch.stack(w)\n",
    "print(w, w.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([5, 2, 2])"
      ]
     },
     "metadata": {},
     "execution_count": 60
    }
   ],
   "source": [
    "result = []\n",
    "for i in range(5):\n",
    "    x = torch.FloatTensor(2, 2)\n",
    "    result += [x]\n",
    "\n",
    "result = torch.stack(result)\n",
    "result.size()"
   ]
  }
 ]
}